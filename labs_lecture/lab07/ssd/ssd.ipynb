{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SSD - Single Shot MultiBox Detector\n",
    "\n",
    "Le R-CNN classificano gli oggetti in due fasi. Nella prima si identificano i potenziali oggetti e nella seconda si associano dell etichette. Queste due fasi sono indipendenti e richiedono dei tempi di computazione più lunghi. Quindi sono stati proposti dei modelli che superano questo tipo di architettura prevedendo un'unica fase di elaborazione dell'immagine in cui si identificano e classificano gli oggetti.\n",
    "\n",
    "Il modello SSD è composto da tre componenti di base:\n",
    "\n",
    "1. Base network, è il modello di base che estrae le feature. Lee scelte più freqeunti riadono su VGG16 oppure ResNet.\n",
    "2. Multi-scale feature layers, sono i livelli che identificano gli oggetti e ne estraggono le feature map.\n",
    "3. Prediction convolutions, sono i livelli che effttuano le predizioni e selezionano gli oggetti candidati più rilevanti.\n",
    "\n",
    "**Architettura SSD**\n",
    "\n",
    "![SSD](SSD-architecture.png)\n",
    "\n",
    "L'output dell'ultimo livello convoluzionale della rete VGG16 è utilizzato come input per i moduli successivi. Questi moduli hanno lo scopo di individuare gli oggetti ai diversi livelli di scala e tutti contribuiscono al risultato finale. In questo senso questa parte di rete è definita come *multiscale*.\n",
    "\n",
    "Come per le R-CNN il risultato dell'inferenza è ottenuto da due layer classificatori. Il primo ritorna la probabilità associata ad ogni classe, la seconda ritorna la quadrupla (x, y, w, h) che identifica il box dell'oggetto. Inoltre nel risultato estratto dalla rete è incluso il valore di *objectness*.\n",
    "\n",
    "La procedura di inferenza di SSD prevede l'individuazione delle *Prior*, cioé gli anchor box alle diverse scale, e per ognuno calcola la predizione. Un ultimo processo di *Non-maximum suppression* (NMS) seleziona i box più prossimi al ground-truth box (i.e. **IoU**).\n",
    "\n",
    "Le Prior vengono espresse in termini di offset, quindi la quadrupla (x, y, w, h) viene riscritta come (g_c_x, g_c_y, g_w, g_h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss\n",
    "\n",
    "Avendo due output, la funzione di loss tiene in considerazione i due contributi.\n",
    "\n",
    "La funzione è definita come\n",
    "\n",
    "![](ssd_loss.png)\n",
    "\n",
    "Con i due contributi definiti come\n",
    "\n",
    "![](ssd_loss_conf.png)\n",
    "\n",
    "![](ssd_loss_loc.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hard Negative Mining\n",
    "\n",
    "Nella loss vengono considerati sia gli esempi positivi (i.e. con overlap) che gli esempi negativi (i.e. senza overlap). Tuttavia il numero di Prior che non contengono oggetti è molto superiore che rende il dataset molto sbilanciato a favore degli esempi negativi. L'idea dell'hard negative mining è utilizzare solo una parte egli esempi negativi in un rapporto prefissato (per SSD è 1:3) con i positivi. In particolare per ogni esempio positivo si selezionano un certo numero di esempi negativi che il modello riconosce come \"difficili\" da classificare, cioè in funzione del valore di Cross Entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('.')\n",
    "\n",
    "from torch import nn\n",
    "from utils import *\n",
    "import torch.nn.functional as F\n",
    "from math import sqrt\n",
    "from itertools import product as product\n",
    "import torchvision\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGBase(nn.Module):\n",
    "    \"\"\"\n",
    "    VGG base convolutions to produce lower-level feature maps.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(VGGBase, self).__init__()\n",
    "\n",
    "        # Standard convolutional layers in VGG16\n",
    "        self.conv1_1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)  # stride = 1, by default\n",
    "        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.conv3_3 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)  # ceiling (not floor) here for even dims\n",
    "\n",
    "        self.conv4_1 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.conv4_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv4_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv5_1 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv5_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv5_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.pool5 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)  # retains size because stride is 1 (and padding)\n",
    "\n",
    "        # Replacements for FC6 and FC7 in VGG16\n",
    "        self.conv6 = nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6)  # atrous convolution\n",
    "\n",
    "        self.conv7 = nn.Conv2d(1024, 1024, kernel_size=1)\n",
    "\n",
    "        # Load pretrained layers\n",
    "        self.load_pretrained_layers()\n",
    "\n",
    "    def forward(self, image):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "\n",
    "        :param image: images, a tensor of dimensions (N, 3, 300, 300)\n",
    "        :return: lower-level feature maps conv4_3 and conv7\n",
    "        \"\"\"\n",
    "        out = F.relu(self.conv1_1(image))  # (N, 64, 300, 300)\n",
    "        out = F.relu(self.conv1_2(out))  # (N, 64, 300, 300)\n",
    "        out = self.pool1(out)  # (N, 64, 150, 150)\n",
    "\n",
    "        out = F.relu(self.conv2_1(out))  # (N, 128, 150, 150)\n",
    "        out = F.relu(self.conv2_2(out))  # (N, 128, 150, 150)\n",
    "        out = self.pool2(out)  # (N, 128, 75, 75)\n",
    "\n",
    "        out = F.relu(self.conv3_1(out))  # (N, 256, 75, 75)\n",
    "        out = F.relu(self.conv3_2(out))  # (N, 256, 75, 75)\n",
    "        out = F.relu(self.conv3_3(out))  # (N, 256, 75, 75)\n",
    "        out = self.pool3(out)  # (N, 256, 38, 38), it would have been 37 if not for ceil_mode = True\n",
    "\n",
    "        out = F.relu(self.conv4_1(out))  # (N, 512, 38, 38)\n",
    "        out = F.relu(self.conv4_2(out))  # (N, 512, 38, 38)\n",
    "        out = F.relu(self.conv4_3(out))  # (N, 512, 38, 38)\n",
    "        conv4_3_feats = out  # (N, 512, 38, 38)\n",
    "        out = self.pool4(out)  # (N, 512, 19, 19)\n",
    "\n",
    "        out = F.relu(self.conv5_1(out))  # (N, 512, 19, 19)\n",
    "        out = F.relu(self.conv5_2(out))  # (N, 512, 19, 19)\n",
    "        out = F.relu(self.conv5_3(out))  # (N, 512, 19, 19)\n",
    "        out = self.pool5(out)  # (N, 512, 19, 19), pool5 does not reduce dimensions\n",
    "\n",
    "        out = F.relu(self.conv6(out))  # (N, 1024, 19, 19)\n",
    "\n",
    "        conv7_feats = F.relu(self.conv7(out))  # (N, 1024, 19, 19)\n",
    "\n",
    "        # Lower-level feature maps\n",
    "        return conv4_3_feats, conv7_feats\n",
    "\n",
    "    def load_pretrained_layers(self):\n",
    "        \"\"\"\n",
    "        As in the paper, we use a VGG-16 pretrained on the ImageNet task as the base network.\n",
    "        There's one available in PyTorch, see https://pytorch.org/docs/stable/torchvision/models.html#torchvision.models.vgg16\n",
    "        We copy these parameters into our network. It's straightforward for conv1 to conv5.\n",
    "        However, the original VGG-16 does not contain the conv6 and con7 layers.\n",
    "        Therefore, we convert fc6 and fc7 into convolutional layers, and subsample by decimation. See 'decimate' in utils.py.\n",
    "        \"\"\"\n",
    "        # Current state of base\n",
    "        state_dict = self.state_dict()\n",
    "        param_names = list(state_dict.keys())\n",
    "\n",
    "        # Pretrained VGG base\n",
    "        pretrained_state_dict = torchvision.models.vgg16(pretrained=True).state_dict()\n",
    "        pretrained_param_names = list(pretrained_state_dict.keys())\n",
    "\n",
    "        # Transfer conv. parameters from pretrained model to current model\n",
    "        for i, param in enumerate(param_names[:-4]):  # excluding conv6 and conv7 parameters\n",
    "            state_dict[param] = pretrained_state_dict[pretrained_param_names[i]]\n",
    "\n",
    "        # Convert fc6, fc7 to convolutional layers, and subsample (by decimation) to sizes of conv6 and conv7\n",
    "        # fc6\n",
    "        conv_fc6_weight = pretrained_state_dict['classifier.0.weight'].view(4096, 512, 7, 7)  # (4096, 512, 7, 7)\n",
    "        conv_fc6_bias = pretrained_state_dict['classifier.0.bias']  # (4096)\n",
    "        state_dict['conv6.weight'] = decimate(conv_fc6_weight, m=[4, None, 3, 3])  # (1024, 512, 3, 3)\n",
    "        state_dict['conv6.bias'] = decimate(conv_fc6_bias, m=[4])  # (1024)\n",
    "        # fc7\n",
    "        conv_fc7_weight = pretrained_state_dict['classifier.3.weight'].view(4096, 4096, 1, 1)  # (4096, 4096, 1, 1)\n",
    "        conv_fc7_bias = pretrained_state_dict['classifier.3.bias']  # (4096)\n",
    "        state_dict['conv7.weight'] = decimate(conv_fc7_weight, m=[4, 4, None, None])  # (1024, 1024, 1, 1)\n",
    "        state_dict['conv7.bias'] = decimate(conv_fc7_bias, m=[4])  # (1024)\n",
    "\n",
    "        # Note: an FC layer of size (K) operating on a flattened version (C*H*W) of a 2D image of size (C, H, W)...\n",
    "        # ...is equivalent to a convolutional layer with kernel size (H, W), input channels C, output channels K...\n",
    "        # ...operating on the 2D image of size (C, H, W) without padding\n",
    "\n",
    "        self.load_state_dict(state_dict)\n",
    "\n",
    "        print(\"\\nLoaded base model.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SSD 300\n",
    "\n",
    "Sono state proposte 2 architetture per l'algoritmo SSD: SSD300 e SSD500. La differenze consiste nella dimensione dell'immagine in input 300x300 vs 500x500 e di conseguenza nelle dimensioni delle feature maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AuxiliaryConvolutions(nn.Module):\n",
    "    \"\"\"\n",
    "    Additional convolutions to produce higher-level feature maps.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(AuxiliaryConvolutions, self).__init__()\n",
    "\n",
    "        # Auxiliary/additional convolutions on top of the VGG base\n",
    "        self.conv8_1 = nn.Conv2d(1024, 256, kernel_size=1, padding=0)  # stride = 1, by default\n",
    "        self.conv8_2 = nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1)  # dim. reduction because stride > 1\n",
    "\n",
    "        self.conv9_1 = nn.Conv2d(512, 128, kernel_size=1, padding=0)\n",
    "        self.conv9_2 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1)  # dim. reduction because stride > 1\n",
    "\n",
    "        self.conv10_1 = nn.Conv2d(256, 128, kernel_size=1, padding=0)\n",
    "        self.conv10_2 = nn.Conv2d(128, 256, kernel_size=3, padding=0)  # dim. reduction because padding = 0\n",
    "\n",
    "        self.conv11_1 = nn.Conv2d(256, 128, kernel_size=1, padding=0)\n",
    "        self.conv11_2 = nn.Conv2d(128, 256, kernel_size=3, padding=0)  # dim. reduction because padding = 0\n",
    "\n",
    "        # Initialize convolutions' parameters\n",
    "        self.init_conv2d()\n",
    "\n",
    "    def init_conv2d(self):\n",
    "        \"\"\"\n",
    "        Initialize convolution parameters.\n",
    "        \"\"\"\n",
    "        for c in self.children():\n",
    "            if isinstance(c, nn.Conv2d):\n",
    "                nn.init.xavier_uniform_(c.weight)\n",
    "                nn.init.constant_(c.bias, 0.)\n",
    "\n",
    "    def forward(self, conv7_feats):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "\n",
    "        :param conv7_feats: lower-level conv7 feature map, a tensor of dimensions (N, 1024, 19, 19)\n",
    "        :return: higher-level feature maps conv8_2, conv9_2, conv10_2, and conv11_2\n",
    "        \"\"\"\n",
    "        out = F.relu(self.conv8_1(conv7_feats))  # (N, 256, 19, 19)\n",
    "        out = F.relu(self.conv8_2(out))  # (N, 512, 10, 10)\n",
    "        conv8_2_feats = out  # (N, 512, 10, 10)\n",
    "\n",
    "        out = F.relu(self.conv9_1(out))  # (N, 128, 10, 10)\n",
    "        out = F.relu(self.conv9_2(out))  # (N, 256, 5, 5)\n",
    "        conv9_2_feats = out  # (N, 256, 5, 5)\n",
    "\n",
    "        out = F.relu(self.conv10_1(out))  # (N, 128, 5, 5)\n",
    "        out = F.relu(self.conv10_2(out))  # (N, 256, 3, 3)\n",
    "        conv10_2_feats = out  # (N, 256, 3, 3)\n",
    "\n",
    "        out = F.relu(self.conv11_1(out))  # (N, 128, 3, 3)\n",
    "        conv11_2_feats = F.relu(self.conv11_2(out))  # (N, 256, 1, 1)\n",
    "\n",
    "        # Higher-level feature maps\n",
    "        return conv8_2_feats, conv9_2_feats, conv10_2_feats, conv11_2_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictionConvolutions(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutions to predict class scores and bounding boxes using lower and higher-level feature maps.\n",
    "\n",
    "    The bounding boxes (locations) are predicted as encoded offsets w.r.t each of the 8732 prior (default) boxes.\n",
    "    See 'cxcy_to_gcxgcy' in utils.py for the encoding definition.\n",
    "\n",
    "    The class scores represent the scores of each object class in each of the 8732 bounding boxes located.\n",
    "    A high score for 'background' = no object.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_classes):\n",
    "        \"\"\"\n",
    "        :param n_classes: number of different types of objects\n",
    "        \"\"\"\n",
    "        super(PredictionConvolutions, self).__init__()\n",
    "\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        # Number of prior-boxes we are considering per position in each feature map\n",
    "        n_boxes = {'conv4_3': 4,\n",
    "                   'conv7': 6,\n",
    "                   'conv8_2': 6,\n",
    "                   'conv9_2': 6,\n",
    "                   'conv10_2': 4,\n",
    "                   'conv11_2': 4}\n",
    "        # 4 prior-boxes implies we use 4 different aspect ratios, etc.\n",
    "\n",
    "        # Localization prediction convolutions (predict offsets w.r.t prior-boxes)\n",
    "        self.loc_conv4_3 = nn.Conv2d(512, n_boxes['conv4_3'] * 4, kernel_size=3, padding=1)\n",
    "        self.loc_conv7 = nn.Conv2d(1024, n_boxes['conv7'] * 4, kernel_size=3, padding=1)\n",
    "        self.loc_conv8_2 = nn.Conv2d(512, n_boxes['conv8_2'] * 4, kernel_size=3, padding=1)\n",
    "        self.loc_conv9_2 = nn.Conv2d(256, n_boxes['conv9_2'] * 4, kernel_size=3, padding=1)\n",
    "        self.loc_conv10_2 = nn.Conv2d(256, n_boxes['conv10_2'] * 4, kernel_size=3, padding=1)\n",
    "        self.loc_conv11_2 = nn.Conv2d(256, n_boxes['conv11_2'] * 4, kernel_size=3, padding=1)\n",
    "\n",
    "        # Class prediction convolutions (predict classes in localization boxes)\n",
    "        self.cl_conv4_3 = nn.Conv2d(512, n_boxes['conv4_3'] * n_classes, kernel_size=3, padding=1)\n",
    "        self.cl_conv7 = nn.Conv2d(1024, n_boxes['conv7'] * n_classes, kernel_size=3, padding=1)\n",
    "        self.cl_conv8_2 = nn.Conv2d(512, n_boxes['conv8_2'] * n_classes, kernel_size=3, padding=1)\n",
    "        self.cl_conv9_2 = nn.Conv2d(256, n_boxes['conv9_2'] * n_classes, kernel_size=3, padding=1)\n",
    "        self.cl_conv10_2 = nn.Conv2d(256, n_boxes['conv10_2'] * n_classes, kernel_size=3, padding=1)\n",
    "        self.cl_conv11_2 = nn.Conv2d(256, n_boxes['conv11_2'] * n_classes, kernel_size=3, padding=1)\n",
    "\n",
    "        # Initialize convolutions' parameters\n",
    "        self.init_conv2d()\n",
    "\n",
    "    def init_conv2d(self):\n",
    "        \"\"\"\n",
    "        Initialize convolution parameters.\n",
    "        \"\"\"\n",
    "        for c in self.children():\n",
    "            if isinstance(c, nn.Conv2d):\n",
    "                nn.init.xavier_uniform_(c.weight)\n",
    "                nn.init.constant_(c.bias, 0.)\n",
    "\n",
    "    def forward(self, conv4_3_feats, conv7_feats, conv8_2_feats, conv9_2_feats, conv10_2_feats, conv11_2_feats):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "\n",
    "        :param conv4_3_feats: conv4_3 feature map, a tensor of dimensions (N, 512, 38, 38)\n",
    "        :param conv7_feats: conv7 feature map, a tensor of dimensions (N, 1024, 19, 19)\n",
    "        :param conv8_2_feats: conv8_2 feature map, a tensor of dimensions (N, 512, 10, 10)\n",
    "        :param conv9_2_feats: conv9_2 feature map, a tensor of dimensions (N, 256, 5, 5)\n",
    "        :param conv10_2_feats: conv10_2 feature map, a tensor of dimensions (N, 256, 3, 3)\n",
    "        :param conv11_2_feats: conv11_2 feature map, a tensor of dimensions (N, 256, 1, 1)\n",
    "        :return: 8732 locations and class scores (i.e. w.r.t each prior box) for each image\n",
    "        \"\"\"\n",
    "        batch_size = conv4_3_feats.size(0)\n",
    "\n",
    "        # Predict localization boxes' bounds (as offsets w.r.t prior-boxes)\n",
    "        l_conv4_3 = self.loc_conv4_3(conv4_3_feats)  # (N, 16, 38, 38)\n",
    "        l_conv4_3 = l_conv4_3.permute(0, 2, 3,\n",
    "                                      1).contiguous()  # (N, 38, 38, 16), to match prior-box order (after .view())\n",
    "        # (.contiguous() ensures it is stored in a contiguous chunk of memory, needed for .view() below)\n",
    "        l_conv4_3 = l_conv4_3.view(batch_size, -1, 4)  # (N, 5776, 4), there are a total 5776 boxes on this feature map\n",
    "\n",
    "        l_conv7 = self.loc_conv7(conv7_feats)  # (N, 24, 19, 19)\n",
    "        l_conv7 = l_conv7.permute(0, 2, 3, 1).contiguous()  # (N, 19, 19, 24)\n",
    "        l_conv7 = l_conv7.view(batch_size, -1, 4)  # (N, 2166, 4), there are a total 2116 boxes on this feature map\n",
    "\n",
    "        l_conv8_2 = self.loc_conv8_2(conv8_2_feats)  # (N, 24, 10, 10)\n",
    "        l_conv8_2 = l_conv8_2.permute(0, 2, 3, 1).contiguous()  # (N, 10, 10, 24)\n",
    "        l_conv8_2 = l_conv8_2.view(batch_size, -1, 4)  # (N, 600, 4)\n",
    "\n",
    "        l_conv9_2 = self.loc_conv9_2(conv9_2_feats)  # (N, 24, 5, 5)\n",
    "        l_conv9_2 = l_conv9_2.permute(0, 2, 3, 1).contiguous()  # (N, 5, 5, 24)\n",
    "        l_conv9_2 = l_conv9_2.view(batch_size, -1, 4)  # (N, 150, 4)\n",
    "\n",
    "        l_conv10_2 = self.loc_conv10_2(conv10_2_feats)  # (N, 16, 3, 3)\n",
    "        l_conv10_2 = l_conv10_2.permute(0, 2, 3, 1).contiguous()  # (N, 3, 3, 16)\n",
    "        l_conv10_2 = l_conv10_2.view(batch_size, -1, 4)  # (N, 36, 4)\n",
    "\n",
    "        l_conv11_2 = self.loc_conv11_2(conv11_2_feats)  # (N, 16, 1, 1)\n",
    "        l_conv11_2 = l_conv11_2.permute(0, 2, 3, 1).contiguous()  # (N, 1, 1, 16)\n",
    "        l_conv11_2 = l_conv11_2.view(batch_size, -1, 4)  # (N, 4, 4)\n",
    "\n",
    "        # Predict classes in localization boxes\n",
    "        c_conv4_3 = self.cl_conv4_3(conv4_3_feats)  # (N, 4 * n_classes, 38, 38)\n",
    "        c_conv4_3 = c_conv4_3.permute(0, 2, 3,\n",
    "                                      1).contiguous()  # (N, 38, 38, 4 * n_classes), to match prior-box order (after .view())\n",
    "        c_conv4_3 = c_conv4_3.view(batch_size, -1,\n",
    "                                   self.n_classes)  # (N, 5776, n_classes), there are a total 5776 boxes on this feature map\n",
    "\n",
    "        c_conv7 = self.cl_conv7(conv7_feats)  # (N, 6 * n_classes, 19, 19)\n",
    "        c_conv7 = c_conv7.permute(0, 2, 3, 1).contiguous()  # (N, 19, 19, 6 * n_classes)\n",
    "        c_conv7 = c_conv7.view(batch_size, -1,\n",
    "                               self.n_classes)  # (N, 2166, n_classes), there are a total 2116 boxes on this feature map\n",
    "\n",
    "        c_conv8_2 = self.cl_conv8_2(conv8_2_feats)  # (N, 6 * n_classes, 10, 10)\n",
    "        c_conv8_2 = c_conv8_2.permute(0, 2, 3, 1).contiguous()  # (N, 10, 10, 6 * n_classes)\n",
    "        c_conv8_2 = c_conv8_2.view(batch_size, -1, self.n_classes)  # (N, 600, n_classes)\n",
    "\n",
    "        c_conv9_2 = self.cl_conv9_2(conv9_2_feats)  # (N, 6 * n_classes, 5, 5)\n",
    "        c_conv9_2 = c_conv9_2.permute(0, 2, 3, 1).contiguous()  # (N, 5, 5, 6 * n_classes)\n",
    "        c_conv9_2 = c_conv9_2.view(batch_size, -1, self.n_classes)  # (N, 150, n_classes)\n",
    "\n",
    "        c_conv10_2 = self.cl_conv10_2(conv10_2_feats)  # (N, 4 * n_classes, 3, 3)\n",
    "        c_conv10_2 = c_conv10_2.permute(0, 2, 3, 1).contiguous()  # (N, 3, 3, 4 * n_classes)\n",
    "        c_conv10_2 = c_conv10_2.view(batch_size, -1, self.n_classes)  # (N, 36, n_classes)\n",
    "\n",
    "        c_conv11_2 = self.cl_conv11_2(conv11_2_feats)  # (N, 4 * n_classes, 1, 1)\n",
    "        c_conv11_2 = c_conv11_2.permute(0, 2, 3, 1).contiguous()  # (N, 1, 1, 4 * n_classes)\n",
    "        c_conv11_2 = c_conv11_2.view(batch_size, -1, self.n_classes)  # (N, 4, n_classes)\n",
    "\n",
    "        # A total of 8732 boxes\n",
    "        # Concatenate in this specific order (i.e. must match the order of the prior-boxes)\n",
    "        locs = torch.cat([l_conv4_3, l_conv7, l_conv8_2, l_conv9_2, l_conv10_2, l_conv11_2], dim=1)  # (N, 8732, 4)\n",
    "        classes_scores = torch.cat([c_conv4_3, c_conv7, c_conv8_2, c_conv9_2, c_conv10_2, c_conv11_2],\n",
    "                                   dim=1)  # (N, 8732, n_classes)\n",
    "\n",
    "        return locs, classes_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSD300(nn.Module):\n",
    "    \"\"\"\n",
    "    The SSD300 network - encapsulates the base VGG network, auxiliary, and prediction convolutions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_classes):\n",
    "        super(SSD300, self).__init__()\n",
    "\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        self.base = VGGBase()\n",
    "        self.aux_convs = AuxiliaryConvolutions()\n",
    "        self.pred_convs = PredictionConvolutions(n_classes)\n",
    "\n",
    "        # Since lower level features (conv4_3_feats) have considerably larger scales, we take the L2 norm and rescale\n",
    "        # Rescale factor is initially set at 20, but is learned for each channel during back-prop\n",
    "        self.rescale_factors = nn.Parameter(torch.FloatTensor(1, 512, 1, 1))  # there are 512 channels in conv4_3_feats\n",
    "        nn.init.constant_(self.rescale_factors, 20)\n",
    "\n",
    "        # Prior boxes\n",
    "        self.priors_cxcy = self.create_prior_boxes()\n",
    "\n",
    "    def forward(self, image):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "\n",
    "        :param image: images, a tensor of dimensions (N, 3, 300, 300)\n",
    "        :return: 8732 locations and class scores (i.e. w.r.t each prior box) for each image\n",
    "        \"\"\"\n",
    "        # Run VGG base network convolutions (lower level feature map generators)\n",
    "        conv4_3_feats, conv7_feats = self.base(image)  # (N, 512, 38, 38), (N, 1024, 19, 19)\n",
    "\n",
    "        # Rescale conv4_3 after L2 norm\n",
    "        norm = conv4_3_feats.pow(2).sum(dim=1, keepdim=True).sqrt()  # (N, 1, 38, 38)\n",
    "        conv4_3_feats = conv4_3_feats / norm  # (N, 512, 38, 38)\n",
    "        conv4_3_feats = conv4_3_feats * self.rescale_factors  # (N, 512, 38, 38)\n",
    "        # (PyTorch autobroadcasts singleton dimensions during arithmetic)\n",
    "\n",
    "        # Run auxiliary convolutions (higher level feature map generators)\n",
    "        conv8_2_feats, conv9_2_feats, conv10_2_feats, conv11_2_feats = \\\n",
    "            self.aux_convs(conv7_feats)  # (N, 512, 10, 10),  (N, 256, 5, 5), (N, 256, 3, 3), (N, 256, 1, 1)\n",
    "\n",
    "        # Run prediction convolutions (predict offsets w.r.t prior-boxes and classes in each resulting localization box)\n",
    "        locs, classes_scores = self.pred_convs(conv4_3_feats, conv7_feats, conv8_2_feats, conv9_2_feats, conv10_2_feats,\n",
    "                                               conv11_2_feats)  # (N, 8732, 4), (N, 8732, n_classes)\n",
    "\n",
    "        return locs, classes_scores\n",
    "\n",
    "    def create_prior_boxes(self):\n",
    "        \"\"\"\n",
    "        Create the 8732 prior (default) boxes for the SSD300, as defined in the paper.\n",
    "\n",
    "        :return: prior boxes in center-size coordinates, a tensor of dimensions (8732, 4)\n",
    "        \"\"\"\n",
    "        fmap_dims = {'conv4_3': 38,\n",
    "                     'conv7': 19,\n",
    "                     'conv8_2': 10,\n",
    "                     'conv9_2': 5,\n",
    "                     'conv10_2': 3,\n",
    "                     'conv11_2': 1}\n",
    "\n",
    "        obj_scales = {'conv4_3': 0.1,\n",
    "                      'conv7': 0.2,\n",
    "                      'conv8_2': 0.375,\n",
    "                      'conv9_2': 0.55,\n",
    "                      'conv10_2': 0.725,\n",
    "                      'conv11_2': 0.9}\n",
    "\n",
    "        aspect_ratios = {'conv4_3': [1., 2., 0.5],\n",
    "                         'conv7': [1., 2., 3., 0.5, .333],\n",
    "                         'conv8_2': [1., 2., 3., 0.5, .333],\n",
    "                         'conv9_2': [1., 2., 3., 0.5, .333],\n",
    "                         'conv10_2': [1., 2., 0.5],\n",
    "                         'conv11_2': [1., 2., 0.5]}\n",
    "\n",
    "        fmaps = list(fmap_dims.keys())\n",
    "\n",
    "        prior_boxes = []\n",
    "\n",
    "        for k, fmap in enumerate(fmaps):\n",
    "            for i in range(fmap_dims[fmap]):\n",
    "                for j in range(fmap_dims[fmap]):\n",
    "                    cx = (j + 0.5) / fmap_dims[fmap]\n",
    "                    cy = (i + 0.5) / fmap_dims[fmap]\n",
    "\n",
    "                    for ratio in aspect_ratios[fmap]:\n",
    "                        prior_boxes.append([cx, cy, obj_scales[fmap] * sqrt(ratio), obj_scales[fmap] / sqrt(ratio)])\n",
    "\n",
    "                        # For an aspect ratio of 1, use an additional prior whose scale is the geometric mean of the\n",
    "                        # scale of the current feature map and the scale of the next feature map\n",
    "                        if ratio == 1.:\n",
    "                            try:\n",
    "                                additional_scale = sqrt(obj_scales[fmap] * obj_scales[fmaps[k + 1]])\n",
    "                            # For the last feature map, there is no \"next\" feature map\n",
    "                            except IndexError:\n",
    "                                additional_scale = 1.\n",
    "                            prior_boxes.append([cx, cy, additional_scale, additional_scale])\n",
    "\n",
    "        prior_boxes = torch.FloatTensor(prior_boxes).to(device)  # (8732, 4)\n",
    "        prior_boxes.clamp_(0, 1)  # (8732, 4)\n",
    "\n",
    "        return prior_boxes\n",
    "\n",
    "    def detect_objects(self, predicted_locs, predicted_scores, min_score, max_overlap, top_k):\n",
    "        \"\"\"\n",
    "        Decipher the 8732 locations and class scores (output of ths SSD300) to detect objects.\n",
    "\n",
    "        For each class, perform Non-Maximum Suppression (NMS) on boxes that are above a minimum threshold.\n",
    "\n",
    "        :param predicted_locs: predicted locations/boxes w.r.t the 8732 prior boxes, a tensor of dimensions (N, 8732, 4)\n",
    "        :param predicted_scores: class scores for each of the encoded locations/boxes, a tensor of dimensions (N, 8732, n_classes)\n",
    "        :param min_score: minimum threshold for a box to be considered a match for a certain class\n",
    "        :param max_overlap: maximum overlap two boxes can have so that the one with the lower score is not suppressed via NMS\n",
    "        :param top_k: if there are a lot of resulting detection across all classes, keep only the top 'k'\n",
    "        :return: detections (boxes, labels, and scores), lists of length batch_size\n",
    "        \"\"\"\n",
    "        batch_size = predicted_locs.size(0)\n",
    "        n_priors = self.priors_cxcy.size(0)\n",
    "        predicted_scores = F.softmax(predicted_scores, dim=2)  # (N, 8732, n_classes)\n",
    "\n",
    "        # Lists to store final predicted boxes, labels, and scores for all images\n",
    "        all_images_boxes = list()\n",
    "        all_images_labels = list()\n",
    "        all_images_scores = list()\n",
    "\n",
    "        assert n_priors == predicted_locs.size(1) == predicted_scores.size(1)\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            # Decode object coordinates from the form we regressed predicted boxes to\n",
    "            decoded_locs = cxcy_to_xy(\n",
    "                gcxgcy_to_cxcy(predicted_locs[i], self.priors_cxcy))  # (8732, 4), these are fractional pt. coordinates\n",
    "\n",
    "            # Lists to store boxes and scores for this image\n",
    "            image_boxes = list()\n",
    "            image_labels = list()\n",
    "            image_scores = list()\n",
    "\n",
    "            max_scores, best_label = predicted_scores[i].max(dim=1)  # (8732)\n",
    "\n",
    "            # Check for each class\n",
    "            for c in range(1, self.n_classes):\n",
    "                # Keep only predicted boxes and scores where scores for this class are above the minimum score\n",
    "                class_scores = predicted_scores[i][:, c]  # (8732)\n",
    "                score_above_min_score = class_scores > min_score  # torch.uint8 (byte) tensor, for indexing\n",
    "                n_above_min_score = score_above_min_score.sum().item()\n",
    "                if n_above_min_score == 0:\n",
    "                    continue\n",
    "                class_scores = class_scores[score_above_min_score]  # (n_qualified), n_min_score <= 8732\n",
    "                class_decoded_locs = decoded_locs[score_above_min_score]  # (n_qualified, 4)\n",
    "\n",
    "                # Sort predicted boxes and scores by scores\n",
    "                class_scores, sort_ind = class_scores.sort(dim=0, descending=True)  # (n_qualified), (n_min_score)\n",
    "                class_decoded_locs = class_decoded_locs[sort_ind]  # (n_min_score, 4)\n",
    "\n",
    "                # Find the overlap between predicted boxes\n",
    "                overlap = find_jaccard_overlap(class_decoded_locs, class_decoded_locs)  # (n_qualified, n_min_score)\n",
    "\n",
    "                # Non-Maximum Suppression (NMS)\n",
    "\n",
    "                # A torch.uint8 (byte) tensor to keep track of which predicted boxes to suppress\n",
    "                # 1 implies suppress, 0 implies don't suppress\n",
    "                suppress = torch.zeros((n_above_min_score), dtype=torch.uint8).to(device)  # (n_qualified)\n",
    "\n",
    "                # Consider each box in order of decreasing scores\n",
    "                for box in range(class_decoded_locs.size(0)):\n",
    "                    # If this box is already marked for suppression\n",
    "                    if suppress[box] == 1:\n",
    "                        continue\n",
    "\n",
    "                    # Suppress boxes whose overlaps (with this box) are greater than maximum overlap\n",
    "                    # Find such boxes and update suppress indices\n",
    "                    suppress = torch.max(suppress, overlap[box] > max_overlap)\n",
    "                    # The max operation retains previously suppressed boxes, like an 'OR' operation\n",
    "\n",
    "                    # Don't suppress this box, even though it has an overlap of 1 with itself\n",
    "                    suppress[box] = 0\n",
    "\n",
    "                # Store only unsuppressed boxes for this class\n",
    "                image_boxes.append(class_decoded_locs[1 - suppress])\n",
    "                image_labels.append(torch.LongTensor((1 - suppress).sum().item() * [c]).to(device))\n",
    "                image_scores.append(class_scores[1 - suppress])\n",
    "\n",
    "            # If no object in any class is found, store a placeholder for 'background'\n",
    "            if len(image_boxes) == 0:\n",
    "                image_boxes.append(torch.FloatTensor([[0., 0., 1., 1.]]).to(device))\n",
    "                image_labels.append(torch.LongTensor([0]).to(device))\n",
    "                image_scores.append(torch.FloatTensor([0.]).to(device))\n",
    "\n",
    "            # Concatenate into single tensors\n",
    "            image_boxes = torch.cat(image_boxes, dim=0)  # (n_objects, 4)\n",
    "            image_labels = torch.cat(image_labels, dim=0)  # (n_objects)\n",
    "            image_scores = torch.cat(image_scores, dim=0)  # (n_objects)\n",
    "            n_objects = image_scores.size(0)\n",
    "\n",
    "            # Keep only the top k objects\n",
    "            if n_objects > top_k:\n",
    "                image_scores, sort_ind = image_scores.sort(dim=0, descending=True)\n",
    "                image_scores = image_scores[:top_k]  # (top_k)\n",
    "                image_boxes = image_boxes[sort_ind][:top_k]  # (top_k, 4)\n",
    "                image_labels = image_labels[sort_ind][:top_k]  # (top_k)\n",
    "\n",
    "            # Append to lists that store predicted boxes and scores for all images\n",
    "            all_images_boxes.append(image_boxes)\n",
    "            all_images_labels.append(image_labels)\n",
    "            all_images_scores.append(image_scores)\n",
    "\n",
    "        return all_images_boxes, all_images_labels, all_images_scores  # lists of length batch_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiBoxLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    The MultiBox loss, a loss function for object detection.\n",
    "\n",
    "    This is a combination of:\n",
    "    (1) a localization loss for the predicted locations of the boxes, and\n",
    "    (2) a confidence loss for the predicted class scores.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, priors_cxcy, threshold=0.5, neg_pos_ratio=3, alpha=1.):\n",
    "        super(MultiBoxLoss, self).__init__()\n",
    "        self.priors_cxcy = priors_cxcy\n",
    "        self.priors_xy = cxcy_to_xy(priors_cxcy)\n",
    "        self.threshold = threshold\n",
    "        self.neg_pos_ratio = neg_pos_ratio\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.smooth_l1 = nn.L1Loss()\n",
    "        self.cross_entropy = nn.CrossEntropyLoss(reduce=False)\n",
    "\n",
    "    def forward(self, predicted_locs, predicted_scores, boxes, labels):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "\n",
    "        :param predicted_locs: predicted locations/boxes w.r.t the 8732 prior boxes, a tensor of dimensions (N, 8732, 4)\n",
    "        :param predicted_scores: class scores for each of the encoded locations/boxes, a tensor of dimensions (N, 8732, n_classes)\n",
    "        :param boxes: true  object bounding boxes in boundary coordinates, a list of N tensors\n",
    "        :param labels: true object labels, a list of N tensors\n",
    "        :return: multibox loss, a scalar\n",
    "        \"\"\"\n",
    "        batch_size = predicted_locs.size(0)\n",
    "        n_priors = self.priors_cxcy.size(0)\n",
    "        n_classes = predicted_scores.size(2)\n",
    "\n",
    "        assert n_priors == predicted_locs.size(1) == predicted_scores.size(1)\n",
    "\n",
    "        true_locs = torch.zeros((batch_size, n_priors, 4), dtype=torch.float).to(device)  # (N, 8732, 4)\n",
    "        true_classes = torch.zeros((batch_size, n_priors), dtype=torch.long).to(device)  # (N, 8732)\n",
    "\n",
    "        # For each image\n",
    "        for i in range(batch_size):\n",
    "            n_objects = boxes[i].size(0)\n",
    "\n",
    "            overlap = find_jaccard_overlap(boxes[i],\n",
    "                                           self.priors_xy)  # (n_objects, 8732)\n",
    "\n",
    "            # For each prior, find the object that has the maximum overlap\n",
    "            overlap_for_each_prior, object_for_each_prior = overlap.max(dim=0)  # (8732)\n",
    "\n",
    "            # We don't want a situation where an object is not represented in our positive (non-background) priors -\n",
    "            # 1. An object might not be the best object for all priors, and is therefore not in object_for_each_prior.\n",
    "            # 2. All priors with the object may be assigned as background based on the threshold (0.5).\n",
    "\n",
    "            # To remedy this -\n",
    "            # First, find the prior that has the maximum overlap for each object.\n",
    "            _, prior_for_each_object = overlap.max(dim=1)  # (N_o)\n",
    "\n",
    "            # Then, assign each object to the corresponding maximum-overlap-prior. (This fixes 1.)\n",
    "            object_for_each_prior[prior_for_each_object] = torch.LongTensor(range(n_objects)).to(device)\n",
    "\n",
    "            # To ensure these priors qualify, artificially give them an overlap of greater than 0.5. (This fixes 2.)\n",
    "            overlap_for_each_prior[prior_for_each_object] = 1.\n",
    "\n",
    "            # Labels for each prior\n",
    "            label_for_each_prior = labels[i][object_for_each_prior]  # (8732)\n",
    "            # Set priors whose overlaps with objects are less than the threshold to be background (no object)\n",
    "            label_for_each_prior[overlap_for_each_prior < self.threshold] = 0  # (8732)\n",
    "\n",
    "            # Store\n",
    "            true_classes[i] = label_for_each_prior\n",
    "\n",
    "            # Encode center-size object coordinates into the form we regressed predicted boxes to\n",
    "            true_locs[i] = cxcy_to_gcxgcy(xy_to_cxcy(boxes[i][object_for_each_prior]), self.priors_cxcy)  # (8732, 4)\n",
    "\n",
    "        # Identify priors that are positive (object/non-background)\n",
    "        positive_priors = true_classes != 0  # (N, 8732)\n",
    "\n",
    "        # LOCALIZATION LOSS\n",
    "\n",
    "        # Localization loss is computed only over positive (non-background) priors\n",
    "        loc_loss = self.smooth_l1(predicted_locs[positive_priors], true_locs[positive_priors])  # (), scalar\n",
    "\n",
    "        # Note: indexing with a torch.uint8 (byte) tensor flattens the tensor when indexing is across multiple dimensions (N & 8732)\n",
    "        # So, if predicted_locs has the shape (N, 8732, 4), predicted_locs[positive_priors] will have (total positives, 4)\n",
    "\n",
    "        # CONFIDENCE LOSS\n",
    "\n",
    "        # Confidence loss is computed over positive priors and the most difficult (hardest) negative priors in each image\n",
    "        # That is, FOR EACH IMAGE,\n",
    "        # we will take the hardest (neg_pos_ratio * n_positives) negative priors, i.e where there is maximum loss\n",
    "        # This is called Hard Negative Mining - it concentrates on hardest negatives in each image, and also minimizes pos/neg imbalance\n",
    "\n",
    "        # Number of positive and hard-negative priors per image\n",
    "        n_positives = positive_priors.sum(dim=1)  # (N)\n",
    "        n_hard_negatives = self.neg_pos_ratio * n_positives  # (N)\n",
    "\n",
    "        # First, find the loss for all priors\n",
    "        conf_loss_all = self.cross_entropy(predicted_scores.view(-1, n_classes), true_classes.view(-1))  # (N * 8732)\n",
    "        conf_loss_all = conf_loss_all.view(batch_size, n_priors)  # (N, 8732)\n",
    "\n",
    "        # We already know which priors are positive\n",
    "        conf_loss_pos = conf_loss_all[positive_priors]  # (sum(n_positives))\n",
    "\n",
    "        # Next, find which priors are hard-negative\n",
    "        # To do this, sort ONLY negative priors in each image in order of decreasing loss and take top n_hard_negatives\n",
    "        conf_loss_neg = conf_loss_all.clone()  # (N, 8732)\n",
    "        conf_loss_neg[positive_priors] = 0.  # (N, 8732), positive priors are ignored (never in top n_hard_negatives)\n",
    "        conf_loss_neg, _ = conf_loss_neg.sort(dim=1, descending=True)  # (N, 8732), sorted by decreasing hardness\n",
    "        hardness_ranks = torch.LongTensor(range(n_priors)).unsqueeze(0).expand_as(conf_loss_neg).to(device)  # (N, 8732)\n",
    "        hard_negatives = hardness_ranks < n_hard_negatives.unsqueeze(1)  # (N, 8732)\n",
    "        conf_loss_hard_neg = conf_loss_neg[hard_negatives]  # (sum(n_hard_negatives))\n",
    "\n",
    "        # As in the paper, averaged over positive priors only, although computed over both positive and hard-negative priors\n",
    "        conf_loss = (conf_loss_hard_neg.sum() + conf_loss_pos.sum()) / n_positives.sum().float()  # (), scalar\n",
    "\n",
    "        # TOTAL LOSS\n",
    "\n",
    "        return conf_loss + self.alpha * loc_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
